{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from IPython.display import display\n",
    "from time import strftime, gmtime\n",
    "import sagemaker\n",
    "from sagemaker.predictor import csv_serializer\n",
    "# import libraries\n",
    "import boto3, re, sys, math, json, os, sagemaker, urllib.request, random\n",
    "from sagemaker import get_execution_role\n",
    "import numpy as np                                \n",
    "import pandas as pd                               \n",
    "import matplotlib.pyplot as plt                   \n",
    "from IPython.display import Image                 \n",
    "from IPython.display import display               \n",
    "from time import gmtime, strftime                 \n",
    "from sagemaker.predictor import csv_serializer   \n",
    "from pandas.api.types import CategoricalDtype\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success - the MySageMakerInstance is in the us-west-2 region. You will use the 433757028032.dkr.ecr.us-west-2.amazonaws.com/xgboost:latest container for your SageMaker endpoint.\n"
     ]
    }
   ],
   "source": [
    "# Define IAM role\n",
    "role = get_execution_role()\n",
    "prefix = 'xgboost'\n",
    "containers = {'us-west-2': '433757028032.dkr.ecr.us-west-2.amazonaws.com/xgboost:latest',\n",
    "              'us-east-1': '811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest',\n",
    "              'us-east-2': '825641698319.dkr.ecr.us-east-2.amazonaws.com/xgboost:latest',\n",
    "              'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com/xgboost:latest'} # each region has its XGBoost container\n",
    "my_region = boto3.session.Session().region_name # set the region of the instance\n",
    "\n",
    "print(\"Success - the MySageMakerInstance is in the \" + my_region + \" region. You will use the \" + containers[my_region] + \" container for your SageMaker endpoint.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 bucket created successfully\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'hcp-ml-project-tune-weight' # <--- change this variable to a unique name for your bucket\n",
    "bucket = bucket_name\n",
    "s3 = boto3.resource('s3')\n",
    "try:\n",
    "    if  my_region == 'us-east-1':\n",
    "      s3.create_bucket(Bucket=bucket_name)\n",
    "    else: \n",
    "      s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={ 'LocationConstraint': my_region })\n",
    "    print('S3 bucket created successfully')\n",
    "except Exception as e:\n",
    "    print('S3 error: ',e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the downloaded data into train/test/validation files\n",
    "FILE_TRAIN = 'train'\n",
    "FILE_VALIDATION = 'validation'\n",
    "FILE_TEST = 'test'\n",
    "columns = ['WILL_ATTACH_IN_60', 'ORGANIZATION_ID', \n",
    "       'PLAN_TIER', 'BILLING_PLAN_ID', 'CC_FEE', 'ASP', 'INDUSTRY', 'ORGSIZE',\n",
    "       'BANK_CONNECTED', 'BANK_VERIFIED', 'CREATED', 'SCHEDULED', 'OMW',\n",
    "       'STARTED', 'IN_PROGRESS', 'ESTIMATED', 'INVOICED', 'TOTALPAYMENT_COUNT',\n",
    "       'TOTALPAYMENT_AMOUNT', 'ONLINE_BOOKINGS', 'CC_COUNT', 'CC_AMOUNT',\n",
    "       'INSTAPAY_AMOUNT', 'ACH_COUNT', 'CASH_COUNT', 'CASH_AMOUNT',\n",
    "       'CHECK_COUNT', 'CHECK_AMOUNT', 'OTHER_COUNT', 'DAYS_SINCE_ENROLLMENT',\n",
    "       'REASON_TO_ADOPT', 'STAGENAME_PRODUCT', 'CC_COUNT_ALL',\n",
    "       'AVG_TIME_TO_PAYMENT', 'COMMITMENTS_ALL', 'COMMITMENTS', 'CC_PERC',\n",
    "       'CASH_PERC', 'CHECK_PERC', 'OTHER_PERC', 'ACH_PERC']\n",
    "columns_new = ['WILL_ATTACH_IN_60', 'weight', 'ORGANIZATION_ID', \n",
    "       'PLAN_TIER', 'BILLING_PLAN_ID', 'CC_FEE', 'ASP', 'INDUSTRY', 'ORGSIZE',\n",
    "       'BANK_CONNECTED', 'BANK_VERIFIED', 'CREATED', 'SCHEDULED', 'OMW',\n",
    "       'STARTED', 'IN_PROGRESS', 'ESTIMATED', 'INVOICED', 'TOTALPAYMENT_COUNT',\n",
    "       'TOTALPAYMENT_AMOUNT', 'ONLINE_BOOKINGS', 'CC_AMOUNT',\n",
    "       'INSTAPAY_AMOUNT', 'ACH_COUNT', 'CASH_COUNT', 'CASH_AMOUNT',\n",
    "       'CHECK_COUNT', 'CHECK_AMOUNT', 'OTHER_COUNT', 'DAYS_SINCE_ENROLLMENT',\n",
    "       'REASON_TO_ADOPT', 'STAGENAME_PRODUCT', 'CC_COUNT_ALL',\n",
    "       'AVG_TIME_TO_PAYMENT', 'COMMITMENTS_ALL', 'COMMITMENTS', 'CC_PERC',\n",
    "       'CASH_PERC', 'CHECK_PERC', 'OTHER_PERC', 'ACH_PERC']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_group_by_id(df, perc):\n",
    "    groups = df.groupby('ORGANIZATION_ID')\n",
    "\n",
    "    split_perc = int(np.rint(df.shape[0] * perc))\n",
    "    train_groups = set()\n",
    "    test_groups = set()\n",
    "    train_rows = 0\n",
    "    for name, group in groups:\n",
    "        train_rows += group.shape[0]\n",
    "        train_groups.add(name)\n",
    "        if train_rows > split_perc:\n",
    "            break\n",
    "    train = groups.filter(lambda x: x.name in train_groups)\n",
    "    test = groups.filter(lambda x: x.name not in train_groups)\n",
    "    s1 = set(train['ORGANIZATION_ID'].unique())\n",
    "    s2 = set(test['ORGANIZATION_ID'].unique())\n",
    "    print(len(s1.intersection(s2)))\n",
    "    print(train_rows, split_perc)\n",
    "    print(train.shape, test.shape)\n",
    "    return train, test\n",
    "\n",
    "def assign_weight(row):\n",
    "    attach_label = row['WILL_ATTACH_IN_60']\n",
    "    cc_count = row['CC_COUNT']\n",
    "    if attach_label == 1 and cc_count > 0:\n",
    "        return 1\n",
    "    elif attach_label == 0 and cc_count <= 0:\n",
    "        return 1\n",
    "    elif attach_label == 1 and cc_count <= 0:\n",
    "        return 3\n",
    "    else:\n",
    "        return 3\n",
    "    \n",
    "def get_dataset():\n",
    "    try:\n",
    "        df = pd.read_csv('../cc_attach_project.csv',index_col=0)\n",
    "        print('Success: Data loaded into dataframe.')\n",
    "    except Exception as e:\n",
    "        print('Data load error: ',e)\n",
    "    \n",
    "    to_drop = ['ENROLLMENT_DATE', 'DATE']\n",
    "    df.drop(to_drop, inplace=True, axis=1)\n",
    "    # df['ENROLLMENT_DATE'] = pd.to_datetime(df['ENROLLMENT_DATE'])\n",
    "    # df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    df['STAGENAME_PRODUCT'] = df['STAGENAME_PRODUCT'].fillna('not called')\n",
    "    df['INDUSTRY'] = df['INDUSTRY'].fillna('missing')\n",
    "    df['CASH_PERC'] = df['CASH_PERC'].fillna(0.0)  \n",
    "    df['CHECK_PERC'] = df['CHECK_PERC'].fillna(0.0)\n",
    "    df['OTHER_PERC'] = df['OTHER_PERC'].fillna(0.0)\n",
    "    df['ACH_PERC'] = df['ACH_PERC'].fillna(0.0)\n",
    "    df['CC_PERC'] = df['CC_PERC'].fillna(0.0)\n",
    "    # df['BILLING_PLAN_ID'] = df['BILLING_PLAN_ID'].astype('int64')\n",
    "    cat_dtype = CategoricalDtype(categories=(df['PLAN_TIER'].unique()), ordered=True)\n",
    "    df['PLAN_TIER'] = df['PLAN_TIER'].astype(cat_dtype) #small > medium?\n",
    "    df['INDUSTRY'] = df['INDUSTRY'].astype('category')\n",
    "    df['ORGSIZE'] = df['ORGSIZE'].abs()\n",
    "    # df['ORGSIZE'].replace(0,1, inplace=True) #should I do this?\n",
    "\n",
    "    #convert to all floats\n",
    "    # df = df.apply(pd.to_numeric, errors='ignore', downcast='float')\n",
    "\n",
    "    #org_id same means same org!!!\n",
    "#     weight = df.apply(assign_weight, axis=1)\n",
    "\n",
    "#     df.insert(0, 'weight', weight, allow_duplicates=False) #insert weight into the df\n",
    "    df = df.reindex(columns, axis='columns')\n",
    "    df = pd.get_dummies(df)\n",
    "    train, test = train_test_group_by_id(df, 0.8)\n",
    "    val, test = train_test_group_by_id(test, 0.5)\n",
    "    train.drop('ORGANIZATION_ID', inplace=True, axis=1)\n",
    "    val.drop('ORGANIZATION_ID', inplace=True, axis=1)\n",
    "    test.drop('ORGANIZATION_ID', inplace=True, axis=1)\n",
    "    print('ORGANIZATION_ID' in train, 'ORGANIZATION_ID' in val, 'ORGANIZATION_ID' in test)\n",
    "#     df = df.sample(frac=1, replace=False, random_state=1) #Shuffle the df\n",
    "    train = train.sample(frac=1, random_state=123)\n",
    "    val = val.sample(frac=1, random_state=123)\n",
    "    test = test.sample(frac=1, random_state=123)\n",
    "#     train, val, test = np.split(df.sample(frac=1), [int(.8 * len(df)), int(.9 * len(df))])\n",
    "    \n",
    "    print(train.shape, test.shape, val.shape)\n",
    "    \n",
    "    return (train, test, val)\n",
    "\n",
    "def get_dataset_new():\n",
    "    try:\n",
    "        df = pd.read_csv('../cc_attach_project.csv',index_col=0)\n",
    "        print('Success: Data loaded into dataframe.')\n",
    "    except Exception as e:\n",
    "        print('Data load error: ',e)\n",
    "    \n",
    "    # df['ENROLLMENT_DATE'] = pd.to_datetime(df['ENROLLMENT_DATE'])\n",
    "    # df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    df['STAGENAME_PRODUCT'] = df['STAGENAME_PRODUCT'].fillna('not called')\n",
    "    df['INDUSTRY'] = df['INDUSTRY'].fillna('missing')\n",
    "    df['CASH_PERC'] = df['CASH_PERC'].fillna(0.0)  \n",
    "    df['CHECK_PERC'] = df['CHECK_PERC'].fillna(0.0)\n",
    "    df['OTHER_PERC'] = df['OTHER_PERC'].fillna(0.0)\n",
    "    df['ACH_PERC'] = df['ACH_PERC'].fillna(0.0)\n",
    "    df['CC_PERC'] = df['CC_PERC'].fillna(0.0)\n",
    "    # df['BILLING_PLAN_ID'] = df['BILLING_PLAN_ID'].astype('int64')\n",
    "    cat_dtype = CategoricalDtype(categories=(df['PLAN_TIER'].unique()), ordered=True)\n",
    "    df['PLAN_TIER'] = df['PLAN_TIER'].astype(cat_dtype) #small > medium?\n",
    "    df['INDUSTRY'] = df['INDUSTRY'].astype('category')\n",
    "    df['ORGSIZE'] = df['ORGSIZE'].abs()\n",
    "    # df['ORGSIZE'].replace(0,1, inplace=True) #should I do this?\n",
    "\n",
    "    #convert to all floats\n",
    "    # df = df.apply(pd.to_numeric, errors='ignore', downcast='float')\n",
    "\n",
    "    #org_id same means same org!!!\n",
    "    weight = df.apply(assign_weight, axis=1)\n",
    "\n",
    "    df.insert(0, 'weight', weight, allow_duplicates=False) #insert weight into the df\n",
    "    to_drop = ['ENROLLMENT_DATE', 'DATE', 'CC_COUNT']\n",
    "    df.drop(to_drop, inplace=True, axis=1)\n",
    "\n",
    "    df = df.reindex(columns_new, axis='columns')\n",
    "    df = pd.get_dummies(df)\n",
    "    train, test = train_test_group_by_id(df, 0.8)\n",
    "    val, test = train_test_group_by_id(test, 0.5)\n",
    "    train.drop('ORGANIZATION_ID', inplace=True, axis=1)\n",
    "    val.drop(['ORGANIZATION_ID', 'weight'], inplace=True, axis=1)\n",
    "    test.drop('ORGANIZATION_ID', inplace=True, axis=1)\n",
    "    print('ORGANIZATION_ID' in train, 'ORGANIZATION_ID' in val, 'ORGANIZATION_ID' in test)\n",
    "#     df = df.sample(frac=1, replace=False, random_state=1) #Shuffle the df\n",
    "    train = train.sample(frac=1, random_state=123)\n",
    "    val = val.sample(frac=1, random_state=123)\n",
    "    test = test.sample(frac=1, random_state=123)\n",
    "#     train, val, test = np.split(df.sample(frac=1), [int(.8 * len(df)), int(.9 * len(df))])\n",
    "    \n",
    "    print(train.shape, test.shape, val.shape)\n",
    "    \n",
    "    return (train, test, val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test, val = get_dataset()\n",
    "# path = 'features_dummies.txt'\n",
    "# file = open(path, 'w')\n",
    "# with open(path, 'w') as writer:\n",
    "#     for col in train.columns:\n",
    "#         writer.write('{}{}'.format(col, '\\n'))\n",
    "# train.to_csv('../Feature-eng/{}.csv'.format(FILE_TRAIN), header=True, index=False)\n",
    "# test.to_csv('../Feature-eng/{}.csv'.format(FILE_TEST), header=True, index=False)\n",
    "# val.to_csv('../Feature-eng/{}.csv'.format(FILE_VALIDATION), header=True, index=False)\n",
    "\n",
    "# print(train.head())\n",
    "# print(train.columns)\n",
    "# df = pd.read_csv('../cc_attach_project.csv',index_col=0, nrows=20000)\n",
    "# df.columns\n",
    "# test.to_csv('test.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def write_to_s3(fobj, bucket, key):\n",
    "    return boto3.Session(region_name=my_region).resource('s3').Bucket(bucket).Object(key).upload_file(fobj)\n",
    "\n",
    "def upload_to_s3(partition_name, partition):\n",
    "#     num_partition = 4                                 # partition file into 5 parts\n",
    "#     partitions = np.array_split(partition, num_partition)\n",
    "    filename = \"./AWS-files/{}.csv\".format(partition_name)\n",
    "    partition.to_csv(filename, header=False, index=False)\n",
    "    aws_filename = \"{}.csv\".format(partition_name)\n",
    "    key = \"{}/{}/{}\".format(prefix,partition_name,aws_filename)\n",
    "    url = 's3n://{}/{}'.format(bucket, key)\n",
    "    print('Writing to {}'.format(url))\n",
    "    write_to_s3(filename, bucket, key)\n",
    "    print('Done writing to {}'.format(url))\n",
    "\n",
    "def download_from_s3(partition_name, number, filename):\n",
    "    key = \"{}/{}/{}\".format(prefix,partition_name, number)\n",
    "    url = 's3n://{}/{}'.format(bucket, key)\n",
    "    print('Reading from {}'.format(url))\n",
    "    s3 = boto3.resource('s3', region_name = my_region)\n",
    "    s3.Bucket(bucket).download_file(key, filename)\n",
    "    try:\n",
    "        s3.Bucket(bucket).download_file(key, 'mnist.local.test')\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        if e.response['Error']['Code'] == \"404\":\n",
    "            print('The object does not exist at {}.'.format(url))\n",
    "        else:\n",
    "            raise        \n",
    "\n",
    "def upload_data():\n",
    "    train, test, val = get_dataset_new()\n",
    "    partitions = [(FILE_TRAIN, train), (FILE_VALIDATION, val), (FILE_TEST, test)]\n",
    "    for partition_name, partition in partitions:\n",
    "        print('{}: {}'.format(partition_name, partition.shape))\n",
    "        upload_to_s3(partition_name, partition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Data loaded into dataframe.\n",
      "0\n",
      "964500 964490\n",
      "(964500, 171) (241112, 171)\n",
      "0\n",
      "120575 120556\n",
      "(120575, 171) (120537, 171)\n",
      "False False False\n",
      "(964500, 170) (120537, 170) (120575, 169)\n",
      "train: (964500, 170)\n",
      "Writing to s3n://hcp-ml-project-tune-weight/xgboost/train/train.csv\n",
      "Done writing to s3n://hcp-ml-project-tune-weight/xgboost/train/train.csv\n",
      "validation: (120575, 169)\n",
      "Writing to s3n://hcp-ml-project-tune-weight/xgboost/validation/validation.csv\n",
      "Done writing to s3n://hcp-ml-project-tune-weight/xgboost/validation/validation.csv\n",
      "test: (120537, 170)\n",
      "Writing to s3n://hcp-ml-project-tune-weight/xgboost/test/test.csv\n",
      "Done writing to s3n://hcp-ml-project-tune-weight/xgboost/test/test.csv\n"
     ]
    }
   ],
   "source": [
    "upload_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partitions = [(FILE_TRAIN, train), (FILE_VALIDATION, val), (FILE_TEST, test)]\n",
    "# for partition_name in [FILE_TRAIN, FILE_VALIDATION, FILE_TEST]:\n",
    "# #     print('{}: {}'.format(partition_name, partition.shape))\n",
    "#     filename = \"{}.csv\".format(partition_name)\n",
    "#     key = \"{}/{}/{}\".format(prefix,partition_name,filename)\n",
    "#     url = 's3n://{}/{}'.format(bucket, key)\n",
    "#     print('Writing to {}'.format(url))\n",
    "#     write_to_s3(filename, bucket, key)\n",
    "#     print('Done writing to {}'.format(url))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

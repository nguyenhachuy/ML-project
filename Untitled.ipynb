{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "df = pd.read_csv('cc_attach_project.csv', nrows=20000)\n",
    "#org id might matter?\n",
    "\n",
    "to_drop = ['Unnamed: 0', 'ORGANIZATION_ID']\n",
    "df.drop(to_drop, inplace=True, axis=1)\n",
    "df['ENROLLMENT_DATE'] = pd.to_datetime(df['ENROLLMENT_DATE'])\n",
    "df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "df['STAGENAME_PRODUCT'] = df['STAGENAME_PRODUCT'].fillna('not called')\n",
    "df['INDUSTRY'] = df['INDUSTRY'].fillna('missing')\n",
    "df['CASH_PERC'] = df['CASH_PERC'].fillna(0.0)  \n",
    "df['CHECK_PERC'] = df['CHECK_PERC'].fillna(0.0)\n",
    "df['OTHER_PERC'] = df['OTHER_PERC'].fillna(0.0)\n",
    "df['ACH_PERC'] = df['ACH_PERC'].fillna(0.0)\n",
    "df['CC_PERC'] = df['CC_PERC'].fillna(0.0)\n",
    "df['BILLING_PLAN_ID'] = df['BILLING_PLAN_ID'].astype('int64')\n",
    "cat_dtype = CategoricalDtype(categories=(df['PLAN_TIER'].unique()), ordered=True)\n",
    "df['PLAN_TIER'] = df['PLAN_TIER'].astype(cat_dtype) #small > medium?\n",
    "df['INDUSTRY'] = df['INDUSTRY'].astype('category')\n",
    "\n",
    "df['ORGSIZE'] = df['ORGSIZE'].abs()\n",
    "df['ORGSIZE'].replace(0,1, inplace=True) #should I do this?\n",
    "#BANK_CONNECTED is 0 1 is that right?\n",
    "#TOTALPAYMENT_COUNT negative? fill zero? abs?\n",
    "#TOTALPAYMENT_AMOUNT negative\n",
    "#CC_COUNT negative\n",
    "#CC_AMOUNT -\n",
    "#ACH_COUNT -\n",
    "#CASH_COUNT -\n",
    "#REASON_TO_ADOPT is 0 1 categorical?\n",
    "#AVG_TIME_TO_PAYMENT - \n",
    "#perc weird values?\n",
    "# data = df.sample(2000, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data, kind=\"reg\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = data.corr()\n",
    "fig, ax = plt.figure(figsize=(10,10))\n",
    "sns.heatmap(corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is STAGENAME_PRODUCT? REASON_TO_ADOPT? OTHER_COUNT\n",
    "# negative orgsize? ASP? \n",
    "df.dtypes\n",
    "# orgsize should be pos\n",
    "#other_count other forms of payment like venmo, paypal\n",
    "#STAGENAME_PRODUCT = status of case to use credit card\n",
    "#replace the missing values with the median\n",
    "#why is certain data missing? \n",
    "#do indicator variable for missing data/available data -> do corr to see if missing one implies the other\n",
    "#categorical vs ordinal\n",
    "#maybe use nearest neighbor to fill in missing data? \n",
    "#confusion matrix, control treatment (remove the calling feature out) did they randomly \n",
    "#cox proportional hazard model\n",
    "#does calling help? are there types of people that are more likely "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PACKAGES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.api.types import CategoricalDtype    \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    " \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.externals import joblib \n",
    "\n",
    "df = pd.read_csv('cc_attach_project.csv', nrows=20000)\n",
    "#org id might matter?\n",
    "\n",
    "to_drop = ['Unnamed: 0', 'ORGANIZATION_ID', 'ENROLLMENT_DATE', 'DATE']\n",
    "df.drop(to_drop, inplace=True, axis=1)\n",
    "# df['ENROLLMENT_DATE'] = pd.to_datetime(df['ENROLLMENT_DATE'])\n",
    "# df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "df['STAGENAME_PRODUCT'] = df['STAGENAME_PRODUCT'].fillna('not called')\n",
    "df['INDUSTRY'] = df['INDUSTRY'].fillna('missing')\n",
    "df['CASH_PERC'] = df['CASH_PERC'].fillna(0.0)  \n",
    "df['CHECK_PERC'] = df['CHECK_PERC'].fillna(0.0)\n",
    "df['OTHER_PERC'] = df['OTHER_PERC'].fillna(0.0)\n",
    "df['ACH_PERC'] = df['ACH_PERC'].fillna(0.0)\n",
    "df['CC_PERC'] = df['CC_PERC'].fillna(0.0)\n",
    "df['BILLING_PLAN_ID'] = df['BILLING_PLAN_ID'].astype('int64')\n",
    "cat_dtype = CategoricalDtype(categories=(df['PLAN_TIER'].unique()), ordered=True)\n",
    "df['PLAN_TIER'] = df['PLAN_TIER'].astype(cat_dtype) #small > medium?\n",
    "df['INDUSTRY'] = df['INDUSTRY'].astype('category')\n",
    "\n",
    "df['ORGSIZE'] = df['ORGSIZE'].abs()\n",
    "df['ORGSIZE'].replace(0,1, inplace=True) #should I do this?\n",
    "\n",
    "\n",
    "#convert to all floats\n",
    "df = df.apply(pd.to_numeric, errors='ignore', downcast='float')\n",
    "df = pd.get_dummies(df)\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['WILL_ATTACH_IN_60']\n",
    "X = df.drop('WILL_ATTACH_IN_60', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=123, \n",
    "                                                    stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Declare data preprocessing steps\n",
    "pipeline = make_pipeline(preprocessing.StandardScaler(), \n",
    "                         RandomForestRegressor(n_estimators=100))\n",
    "# 6. Declare hyperparameters to tune\n",
    "hyperparameters = { 'randomforestregressor__max_features' : ['auto', 'sqrt', 'log2'],\n",
    "                  'randomforestregressor__max_depth': [None, 5, 3, 1]}\n",
    " \n",
    "# 7. Tune model using cross-validation pipeline\n",
    "clf = GridSearchCV(pipeline, hyperparameters, cv=10)\n",
    " \n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_params_)\n",
    "\n",
    "# 8. Refit on the entire training set\n",
    "# No additional code needed if clf.refit == True (default is True)\n",
    " \n",
    "# 9. Evaluate model pipeline on test data\n",
    "pred = clf.predict(X_test)\n",
    "print(r2_score(y_test, pred))\n",
    "print(mean_squared_error(y_test, pred))\n",
    " \n",
    "# 10. Save model for future use\n",
    "joblib.dump(clf, 'rf_regressor.pkl')\n",
    "# To load: clf2 = joblib.load('rf_regressor.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying out SVM\n",
    "from sklearn.svm import SVC\n",
    "# 5. Declare data preprocessing steps\n",
    "pipeline = make_pipeline(preprocessing.StandardScaler(), \n",
    "                         SVC(gamma='scale', kernel='rbf'))\n",
    "# 6. Declare hyperparameters to tune\n",
    "hyperparameters = { 'svc__C' : [0.1, 1, 10, 100],\n",
    "                  'svc__gamma': [10**-7, 10**-6, 10**-5, 10**-4]\n",
    "                  }\n",
    " \n",
    "# 7. Tune model using cross-validation pipeline\n",
    "clf = GridSearchCV(pipeline, hyperparameters, cv=10)\n",
    " \n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_params_)\n",
    "\n",
    "# 8. Refit on the entire training set\n",
    "# No additional code needed if clf.refit == True (default is True)\n",
    " \n",
    "# 9. Evaluate model pipeline on test data\n",
    "pred = clf.predict(X_test)\n",
    "print(r2_score(y_test, pred))\n",
    "print(mean_squared_error(y_test, pred))\n",
    " \n",
    "# 10. Save model for future use\n",
    "joblib.dump(clf, 'svm_classifier.pkl')\n",
    "# To load: clf2 = joblib.load('rf_regressor.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load: \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "y = df['WILL_ATTACH_IN_60']\n",
    "X = df.drop('WILL_ATTACH_IN_60', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=123, \n",
    "                                                    stratify=y)\n",
    "\n",
    "clf2 = RandomForestClassifier(n_estimators=100, max_depth=None, max_features='sqrt',\n",
    "                              random_state=123)\n",
    "clf2.fit(X_train, y_train)\n",
    "print(clf2.feature_importances_)\n",
    "print(clf2.score(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(12, 10))\n",
    "h = 0.02\n",
    "i = 1\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "cm = plt.cm.jet\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, alpha=0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=1000, n_features=4,\n",
    "                            n_informative=2, n_redundant=0,\n",
    "                            random_state=0, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df:\n",
    "    print(df[column].dtype == np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

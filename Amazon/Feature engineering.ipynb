{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from IPython.display import display\n",
    "from time import strftime, gmtime\n",
    "import sagemaker\n",
    "from sagemaker.predictor import csv_serializer\n",
    "# import libraries\n",
    "import boto3, re, sys, math, json, os, sagemaker, urllib.request, random\n",
    "from sagemaker import get_execution_role\n",
    "import numpy as np                                \n",
    "import pandas as pd                               \n",
    "import matplotlib.pyplot as plt                   \n",
    "from IPython.display import Image                 \n",
    "from IPython.display import display               \n",
    "from time import gmtime, strftime                 \n",
    "from sagemaker.predictor import csv_serializer   \n",
    "from pandas.api.types import CategoricalDtype\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success - the MySageMakerInstance is in the us-west-2 region. You will use the 433757028032.dkr.ecr.us-west-2.amazonaws.com/xgboost:latest container for your SageMaker endpoint.\n"
     ]
    }
   ],
   "source": [
    "# Define IAM role\n",
    "role = get_execution_role()\n",
    "prefix = 'xgboost'\n",
    "containers = {'us-west-2': '433757028032.dkr.ecr.us-west-2.amazonaws.com/xgboost:latest',\n",
    "              'us-east-1': '811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest',\n",
    "              'us-east-2': '825641698319.dkr.ecr.us-east-2.amazonaws.com/xgboost:latest',\n",
    "              'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com/xgboost:latest'} # each region has its XGBoost container\n",
    "my_region = boto3.session.Session().region_name # set the region of the instance\n",
    "\n",
    "print(\"Success - the MySageMakerInstance is in the \" + my_region + \" region. You will use the \" + containers[my_region] + \" container for your SageMaker endpoint.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 error:  An error occurred (BucketAlreadyOwnedByYou) when calling the CreateBucket operation: Your previous request to create the named bucket succeeded and you already own it.\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'hcp-ml-project' # <--- change this variable to a unique name for your bucket\n",
    "bucket = bucket_name\n",
    "s3 = boto3.resource('s3')\n",
    "try:\n",
    "    if  my_region == 'us-east-1':\n",
    "      s3.create_bucket(Bucket=bucket_name)\n",
    "    else: \n",
    "      s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={ 'LocationConstraint': my_region })\n",
    "    print('S3 bucket created successfully')\n",
    "except Exception as e:\n",
    "    print('S3 error: ',e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the downloaded data into train/test/validation files\n",
    "FILE_TRAIN = 'train'\n",
    "FILE_VALIDATION = 'validation'\n",
    "FILE_TEST = 'test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_weight(row):\n",
    "    attach_label = row['WILL_ATTACH_IN_60']\n",
    "    cc_count = row['CC_COUNT']\n",
    "    if attach_label == 1 and cc_count > 0:\n",
    "        return 0.5\n",
    "    elif attach_label == 0 and cc_count <= 0:\n",
    "        return 0.5\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def get_dataset():\n",
    "    try:\n",
    "        df = pd.read_csv('../cc_attach_project.csv',index_col=0)\n",
    "        print('Success: Data loaded into dataframe.')\n",
    "    except Exception as e:\n",
    "        print('Data load error: ',e)\n",
    "    \n",
    "    to_drop = ['ENROLLMENT_DATE', 'DATE']\n",
    "    df.drop(to_drop, inplace=True, axis=1)\n",
    "    # df['ENROLLMENT_DATE'] = pd.to_datetime(df['ENROLLMENT_DATE'])\n",
    "    # df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    df['STAGENAME_PRODUCT'] = df['STAGENAME_PRODUCT'].fillna('not called')\n",
    "    df['INDUSTRY'] = df['INDUSTRY'].fillna('missing')\n",
    "    df['CASH_PERC'] = df['CASH_PERC'].fillna(0.0)  \n",
    "    df['CHECK_PERC'] = df['CHECK_PERC'].fillna(0.0)\n",
    "    df['OTHER_PERC'] = df['OTHER_PERC'].fillna(0.0)\n",
    "    df['ACH_PERC'] = df['ACH_PERC'].fillna(0.0)\n",
    "    df['CC_PERC'] = df['CC_PERC'].fillna(0.0)\n",
    "    # df['BILLING_PLAN_ID'] = df['BILLING_PLAN_ID'].astype('int64')\n",
    "    cat_dtype = CategoricalDtype(categories=(df['PLAN_TIER'].unique()), ordered=True)\n",
    "    df['PLAN_TIER'] = df['PLAN_TIER'].astype(cat_dtype) #small > medium?\n",
    "    df['INDUSTRY'] = df['INDUSTRY'].astype('category')\n",
    "    df['ORGSIZE'] = df['ORGSIZE'].abs()\n",
    "    # df['ORGSIZE'].replace(0,1, inplace=True) #should I do this?\n",
    "\n",
    "    #convert to all floats\n",
    "    # df = df.apply(pd.to_numeric, errors='ignore', downcast='float')\n",
    "\n",
    "    #org_id same means same org!!!\n",
    "    weight = df.apply(assign_weight, axis=1)\n",
    "\n",
    "    df.insert(1, 'weight', weight, allow_duplicates=False) #insert weight into the df\n",
    "    df = df.sample(frac=1, replace=False, random_state=1) #Shuffle the df\n",
    "\n",
    "    df = pd.get_dummies(df)\n",
    "    \n",
    "    print(df.shape)\n",
    "    train, test = train_test_split(df, test_size=0.15, random_state=123)\n",
    "    train, val = train_test_split(train, test_size=0.2, random_state=123)\n",
    "    \n",
    "    print(train.shape, test.shape, val.shape)\n",
    "    \n",
    "    return (train, test, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def write_to_s3(fobj, bucket, key):\n",
    "    return boto3.Session(region_name=my_region).resource('s3').Bucket(bucket).Object(key).upload_file(fobj)\n",
    "\n",
    "def upload_to_s3(partition_name, partition):\n",
    "    num_partition = 4                                 # partition file into 5 parts\n",
    "    partitions = np.array_split(partition, num_partition)\n",
    "    for i in range(num_partition):\n",
    "        filename = \"{}-{}.csv\".format(partition_name, str(i))\n",
    "        partition.to_csv(filename, header=False, index=False)\n",
    "        key = \"{}/{}/{}\".format(prefix,partition_name,filename)\n",
    "        url = 's3n://{}/{}'.format(bucket, key)\n",
    "        print('Writing to {}'.format(url))\n",
    "        write_to_s3(filename, bucket, key)\n",
    "        print('Done writing to {}'.format(url))\n",
    "\n",
    "def download_from_s3(partition_name, number, filename):\n",
    "    key = \"{}/{}/{}\".format(prefix,partition_name, number)\n",
    "    url = 's3n://{}/{}'.format(bucket, key)\n",
    "    print('Reading from {}'.format(url))\n",
    "    s3 = boto3.resource('s3', region_name = my_region)\n",
    "    s3.Bucket(bucket).download_file(key, filename)\n",
    "    try:\n",
    "        s3.Bucket(bucket).download_file(key, 'mnist.local.test')\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        if e.response['Error']['Code'] == \"404\":\n",
    "            print('The object does not exist at {}.'.format(url))\n",
    "        else:\n",
    "            raise        \n",
    "\n",
    "def upload_data():\n",
    "    train, test, val = get_dataset()\n",
    "    partitions = [(FILE_TRAIN, train), (FILE_VALIDATION, val), (FILE_TEST, test)]\n",
    "    for partition_name, partition in partitions:\n",
    "        print('{}: {}'.format(partition_name, partition.shape))\n",
    "        upload_to_s3(partition_name, partition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Data loaded into dataframe.\n",
      "(1205612, 172)\n",
      "(819816, 172) (180842, 172) (204954, 172)\n",
      "train: (819816, 172)\n",
      "Writing to s3n://hcp-ml-project/xgboost/train/train-0.csv\n",
      "Done writing to s3n://hcp-ml-project/xgboost/train/train-0.csv\n",
      "Writing to s3n://hcp-ml-project/xgboost/train/train-1.csv\n",
      "Done writing to s3n://hcp-ml-project/xgboost/train/train-1.csv\n",
      "Writing to s3n://hcp-ml-project/xgboost/train/train-2.csv\n",
      "Done writing to s3n://hcp-ml-project/xgboost/train/train-2.csv\n",
      "Writing to s3n://hcp-ml-project/xgboost/train/train-3.csv\n",
      "Done writing to s3n://hcp-ml-project/xgboost/train/train-3.csv\n",
      "validation: (204954, 172)\n",
      "Writing to s3n://hcp-ml-project/xgboost/validation/validation-0.csv\n",
      "Done writing to s3n://hcp-ml-project/xgboost/validation/validation-0.csv\n",
      "Writing to s3n://hcp-ml-project/xgboost/validation/validation-1.csv\n",
      "Done writing to s3n://hcp-ml-project/xgboost/validation/validation-1.csv\n",
      "Writing to s3n://hcp-ml-project/xgboost/validation/validation-2.csv\n",
      "Done writing to s3n://hcp-ml-project/xgboost/validation/validation-2.csv\n",
      "Writing to s3n://hcp-ml-project/xgboost/validation/validation-3.csv\n",
      "Done writing to s3n://hcp-ml-project/xgboost/validation/validation-3.csv\n",
      "test: (180842, 172)\n",
      "Writing to s3n://hcp-ml-project/xgboost/test/test-0.csv\n",
      "Done writing to s3n://hcp-ml-project/xgboost/test/test-0.csv\n",
      "Writing to s3n://hcp-ml-project/xgboost/test/test-1.csv\n",
      "Done writing to s3n://hcp-ml-project/xgboost/test/test-1.csv\n",
      "Writing to s3n://hcp-ml-project/xgboost/test/test-2.csv\n",
      "Done writing to s3n://hcp-ml-project/xgboost/test/test-2.csv\n",
      "Writing to s3n://hcp-ml-project/xgboost/test/test-3.csv\n",
      "Done writing to s3n://hcp-ml-project/xgboost/test/test-3.csv\n"
     ]
    }
   ],
   "source": [
    "upload_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
